{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# qauntum neural network belief propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the NNBP decode quantum code`[1]`, I implementing this by Pytorch`[2]`. This part focus on the impementation to decode 2D Toric code`[3]`. This note just illustrate the diffence between the decoding classic codes and quantum code, for more detail, one can refer to the NNBP repo.  \n",
    "The main diffence between classic and quantum decoding in computational basis is that the syndrome change the parity check eqaution's result. Also, we introduce a residual connection to overcome the challenge of gradient vanishing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **First Part: define the hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch \n",
    "import error_generate\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "L = 4\n",
    "P = list(np.linspace(0.01, 0.05, num = 6))\n",
    "H = torch.from_numpy(error_generate.generate_PCM(2 * L * L - 2, L))\n",
    "h_prep = error_generate.H_Prep(H)\n",
    "H_prep = torch.from_numpy(h_prep.get_H_Prep())\n",
    "BATCH_SIZE = 120\n",
    "#torch.manual_seed(1)\n",
    "run = 1200\n",
    "lr = 2e-4\n",
    "Nc = 15\n",
    "torch.cuda.set_device(0)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "dataset = error_generate.gen_syn(P, L, H, run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Second Part: dataset and one iteration of the NNBP algorithm**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return dataset[2 * index], dataset[2 * index + 1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(len(self.dataset) / 2)\n",
    "\n",
    "\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, H, resi):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.H = Variable(H, requires_grad = False).cuda()\n",
    "        self.rows, self.cols = H.size()\n",
    "        self.W_check = torch.nn.Parameter((torch.ones((self.rows, self.rows), dtype = torch.float64) - \\\n",
    "                                           torch.eye(self.rows, dtype = torch.float64)))\n",
    "        self.W_vec = torch.nn.Parameter(Variable(torch.ones((self.rows, 1), dtype = torch.float64)))\n",
    "        self.resi = resi\n",
    "        \n",
    "    def forward(self, x):\n",
    "        L, Syn, M_check, results= x\n",
    "        H = (Variable(torch.ones((M_check.size()), dtype = torch.float64))).cuda().mul(self.H)\n",
    "        check = Variable(torch.ones((self.rows, self.rows), dtype = torch.float64) - \\\n",
    "                         torch.eye(self.rows, dtype = torch.float64)).cuda()\n",
    "        var = Variable(torch.ones((self.cols, self.cols), dtype = torch.float64) - \\\n",
    "                       torch.eye(self.cols, dtype = torch.float64)).cuda()\n",
    "        result = M_check\n",
    "        result = torch.where(H.transpose(1, 2) == 0, result.transpose(1, 2), \\\n",
    "                             torch.matmul(result.transpose(1, 2), self.W_check.mul(check)))\n",
    "        result += torch.matmul(L, self.H.t())\n",
    "        result = torch.clamp(result, -10 ,10)\n",
    "        result = torch.where(H.transpose(1, 2) == 0, result, torch.tanh(result / 2))\n",
    "        Coeff = torch.where(result < 0, torch.ones(result.size(), dtype = torch.float64).cuda(), \\\n",
    "                            torch.zeros(result.size(), dtype = torch.float64).cuda())\n",
    "        result = torch.where(H.transpose(1, 2) == 0, result, abs(result))\n",
    "        result = torch.where(H.transpose(1, 2) == 0, result + 3, result)\n",
    "#        print(torch.where(result > 0, torch.zeros(result.size(), dtype = torch.float64).cuda(), result).to_sparse()._values())\n",
    "        result = torch.where(H.transpose(1, 2) == 0, result, result.clamp(1e-40))\n",
    "        result = torch.where(H.transpose(1, 2) == 0, result, torch.log(result))\n",
    "        result = torch.where(H.transpose(1, 2) == 0, result - 3, result)\n",
    "        result = torch.where(H == 0, result.transpose(1, 2), torch.matmul(result.transpose(1, 2), var))\n",
    "#        print(torch.where(result < 0, torch.zeros(result.size(), dtype = torch.float64), result).to_sparse()._values())\n",
    "        result = torch.where(H == 0, result, torch.exp(result))\n",
    "        Coeff = torch.where(H == 0, Coeff.transpose(1, 2), torch.matmul(Coeff.transpose(1, 2), var))\n",
    "        result = torch.where(H == 0, result, torch.cos(math.pi * (Coeff + Syn[:, 0 : self.rows].unsqueeze(2))).mul(result))\n",
    "        result = torch.where(H == 0, result, torch.log((1 + result) / (1 - result)))\n",
    "        '''\n",
    "        As the author suggest, we can introduce residual connect to overcome the challence of gradient vanishing.\n",
    "        '''\n",
    "        if self.resi == 1:\n",
    "            result += M_check\n",
    "        output = result\n",
    "        result = torch.matmul(output.transpose(1, 2), self.W_vec)\n",
    "        result += torch.matmul(L, Variable(torch.ones((self.cols, 1), dtype = torch.float64)).cuda())\n",
    "        results.append(result)\n",
    "        return L, Syn, output, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Third part: neural network belief propagation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNBP(torch.nn.Module):\n",
    "    def __init__(self, H, Nc, resi):\n",
    "        super(NNBP, self).__init__()\n",
    "        self.H = H.cuda()\n",
    "        self.resi = resi\n",
    "        self.rows, self.cols = H.size()\n",
    "        self.Nc = Nc\n",
    "        self.layer = self._make_layer()\n",
    "        \n",
    "    def _make_layer(self):\n",
    "        layers = []\n",
    "        for i in range(self.Nc):\n",
    "            layers.append(ResidualBlock(self.H, self.resi).cuda())\n",
    "        return torch.nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        L = Variable(torch.zeros((x.size()[0], self.cols, self.cols), dtype = torch.float64))\n",
    "        for i in range(x.size()[0]):\n",
    "            L[i] = torch.diag(x[i, 0])\n",
    "        Syn = x[:, 1, :]\n",
    "        L = L.cuda()\n",
    "        Syn = Syn.cuda()\n",
    "        results = []\n",
    "        M_check_init = Variable(torch.zeros(x.size()[0], self.rows, self.cols, dtype = torch.float64)).cuda()\n",
    "        x = (L, Syn, M_check_init, results)\n",
    "        x = self.layer(x)\n",
    "        return x[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Forth Part: loss function and train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunc(torch.nn.Module):\n",
    "    def __init__(self, H_prep):\n",
    "        super(LossFunc, self).__init__()\n",
    "        self.H_prep = Variable(H_prep).cuda()\n",
    "        \n",
    "    def forward(self, result, err):\n",
    "        medium = torch.matmul(self.H_prep, (err.transpose(1, 2) + torch.sigmoid(-1 * result)))\n",
    "        medium = abs(torch.sin(medium * math.pi / 2))\n",
    "        loss = torch.sum(medium)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "def training(H, lr, L, train_num, train, load):\n",
    "    loss_sum = 0\n",
    "    rows, cols  = H.shape\n",
    "    if (train or load):\n",
    "        resi = 1\n",
    "    else:\n",
    "        resi = 0\n",
    "    decoder = NNBP(H, Nc, resi).cuda()\n",
    "#    if load:\n",
    "#        decoder.load_state_dict(torch.load('./model/decoder_parameters_L=%d.pkl' % L))\n",
    "#        for name, param in decoder.named_parameters():\n",
    "#            if param.requires_grad:\n",
    "##                print(name,torch.where(param > 0.0, torch.zeros(param.size(), dtype = torch.float64), param).to_sparse()._values())\n",
    "#                print(name,param)\n",
    "#        return\n",
    "\n",
    "    torch_dataset = CustomDataset(dataset)\n",
    "    loader = Data.DataLoader(\n",
    "        dataset = torch_dataset,      # torch TensorDataset format\n",
    "        batch_size = BATCH_SIZE,      # mini batch size\n",
    "        shuffle = True,               # random shuffle for training\n",
    "        num_workers = 0,              # subprocesses for loading data\n",
    "    )\n",
    "    \n",
    "    criterion = LossFunc(H_prep).cuda()\n",
    "    optimizer = torch.optim.Adam(decoder.parameters(), lr = lr)\n",
    "    for epoch in range(train_num):\n",
    "        print('epoch',epoch)\n",
    "        for step, (data, target) in enumerate(loader):\n",
    "            loss = Variable(torch.zeros((1), dtype = torch.float64)).cuda()\n",
    "            data, target = Variable(data).cuda(), Variable(target).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            results = decoder(data)\n",
    "            if train:\n",
    "                for result in results:\n",
    "                    loss += criterion(result, target)\n",
    "                loss /= len(results)\n",
    "                loss.backward()       \n",
    "                optimizer.step()\n",
    "                for p in decoder.parameters():\n",
    "                    p.data.clamp_(1e-10)\n",
    "            else:\n",
    "                loss_sum += criterion(results[len(results) - 1], target)\n",
    "    if train:\n",
    "        torch.save(decoder.state_dict(), '.\\model\\decoder_parameters_L=%d.pkl' % L)\n",
    "    return loss_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fifth Part: training the neural network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    train = 0\n",
    "    load = 1\n",
    "    train_num = 1\n",
    "    loss = training(H, lr, L, train_num, train, load)\n",
    "    if not train: print(loss.item() / (run * (2 * L ** 2 - 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sixth Part: results and evalaution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waiting for coming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[1]`: Ye-Hua Liu and David Poulin, “Neural belief-propagation decoders for quantum error-correcting codes,” arXiv preprint arXiv:1811.07835 (2018).  \n",
    "`[2]`: https://pytorch.org/  \n",
    "`[3]`: A. Yu. Kitaev, Ann. Phys. (N.Y.) 303, 2 (2003).  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
